{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"hw3.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"0Nbq_oB2NI3E"},"source":["# 546 (Optimization for Learning and Control) hw3\n","\n","You are welcome (and encouraged) to work with others, but each individual must submit their own writeup.\n","\n","You are welcome to use your preferred computational toolkit -- upload the **commented** sourcecode alongside your writeup (e.g. the .ipynb file).\n","\n","You are welcome to consult research articles and other materials -- upload a .pdf of the article alongside your writeup and indicate which references were used where in the writeup."]},{"cell_type":"markdown","metadata":{"id":"1yz1n86PNI3H"},"source":["# Markov decision process (MDP) (14 pts)"]},{"cell_type":"markdown","metadata":{"id":"BoRzaEsNNI3I"},"source":["Let $(X,U,P,c)$ be an MDP:\n","\n","* $X$ is a finite set of $N$ *states*;\n","* $U$ is a finite set of $M$ *actions*;\n","* $P:X\\times U\\rightarrow \\Delta(X)$ specifies transition probabilities;\n","* $C:X\\times U\\rightarrow \\mathbb{R}$ specifies transition cost.\n","\n","Recall that $\\Delta(S) = \\{p : S \\rightarrow [0,1] : \\sum_{s\\in S} p(s) = 1\\}$ is the set of probability distributions over the finite set $S$."]},{"cell_type":"markdown","metadata":{"id":"THQ7pkHANI3I"},"source":["a. *Implement an algorithm that generates a random MDP given finite sets $X$ and $U$ (i.e. let $N = |X|$, $M = |U|$ be parameters that are easy to vary, and generate $P$ and $c$ randomly).*"]},{"cell_type":"markdown","metadata":{"id":"PUvt8PuMNI3J"},"source":["b. *Implement an algorithm that simulates an MDP (i.e. write a function that inputs transition probabilities $P$, cost $c$, initial state distribution $p_0$, policy $\\pi:X\\rightarrow\\Delta(U)$, and time horizon $t$ and returns the state distribution $p_t$).*"]},{"cell_type":"markdown","metadata":{"id":"FZ48dOXuNI3J"},"source":["c. *Test your algorithm from (b.) (i.e. design one or more tests, explain why these tests are exhaustive, and provide summary statistics and/or visualizations that convince a skeptical reader that your algorithm is correct).*"]},{"cell_type":"markdown","metadata":{"id":"sDYop_ybNI3K"},"source":["Let us now consider the problem of minimizing the infinite-horizon discounted cost\n","$$c = \\sum_{t=0}^\\infty \\gamma^t C(x_t,u_t),$$\n","where $\\gamma\\in(0,1)$ is a ***discount factor***."]},{"cell_type":"markdown","metadata":{"id":"91q5DHAwNI3K"},"source":["Any policy $\\pi : X\\rightarrow\\Delta(U)$ has an associated ***value*** $v^\\pi : X\\rightarrow\\mathbb{R}$ defined by\n","$$\\forall x\\in X : v^\\pi(x) = E[c \\mid x = x_0]$$\n","that satisfies the *Bellman equation*\n","$$\\forall x\\in X : v^\\pi(x) = \\sum_{u\\in U}\\pi(u|x)\\sum_{x^+\\in X} P(x^+|x,u)\\left[C(x,u) + \\gamma \\cdot v^\\pi(x^+)\\right].$$"]},{"cell_type":"markdown","metadata":{"id":"_RoYRpTtNI3K"},"source":["d. *Noting that $v^\\pi$ appears linearly in this Bellman equation, implement a ***policy evaluation*** algorithm that computes $v^\\pi$ using linear algebra (i.e. determing $L$ and $b$ such that $L v^\\pi = b$ and use this equation to solve for $v^\\pi$).*"]},{"cell_type":"markdown","metadata":{"id":"K0_oTUisNI3L"},"source":["e. *Test your policy evaluation algorithm from (d.) (i.e. design one or more tests, explain why these tests are exhaustive, and provide summary statistics and/or visualizations that convince a skeptical reader that your algorithm is correct).*"]},{"cell_type":"markdown","metadata":{"id":"Tv56gQ-oNI3L"},"source":["f. *Using your policy evaluation algorithm from (d.), implement ***value iteration*** and ***policy iteration*** algorithms.*"]},{"cell_type":"markdown","metadata":{"id":"_AxUBIT9NI3L"},"source":["g. *Test your value iteration and policy iteration algorithms from (f.) (i.e. design one or more tests, explain why these tests are exhaustive, and provide summary statistics and/or visualizations that convince a skeptical reader that your algorithm is correct).*"]},{"cell_type":"markdown","metadata":{"id":"wPdI3b--NI3M"},"source":["## linear quadratic regulation (6 pts)"]},{"cell_type":"markdown","metadata":{"id":"n0Sz3znwNI3M"},"source":["Consider the linear-quadratic regulation (LQR) problem for the discrete-time linear time-invariant system\n","$$x^+ = A x + B u$$\n","with infinite-horizon cost\n","$$c(x,u) = \\frac{1}{2}\\sum_{t=0}^\\infty x^T Q x + u^T R u \\ dt$$\n","where  $Q = Q^T > 0$ and $R = R^T > 0$.  We know that the optimal policy is linear in state, \n","$$u = - K x,$$\n","where $K = (R + B^T Y B)^{-1} (B^T Y A)$ and $Y$ satisfies the discrete algebraic Riccati equation\n","$$Y = A^T Y A - (A^T Y B)(R + B^T Y B)^{-1}(B^T Y A) + Q.$$"]},{"cell_type":"markdown","metadata":{"id":"LmelWMGTNI3N"},"source":["For this problem, we'll consider a one-dimensional system, so $x,u\\in\\mathbb{R}$, with the values $A, B, Q, R = 1$."]},{"cell_type":"markdown","metadata":{"id":"vBV7m_UrNI3N"},"source":["If the state distribution starts out Gaussian, \n","$$x \\sim \\mathcal{N}(\\xi, \\Sigma),$$\n","and we apply zero input but add a state disturbance with zero mean and covariance $W$, then (so long as the disturbance is uncorrelated with the state) the state distribution after one step is Gaussian:\n","$$x^+ \\sim \\mathcal{N}(A\\xi, A\\Sigma A^T + W),$$\n","i.e. the covariance gets updated via\n","$$\\Sigma^+ = A\\Sigma A^T + W.$$\n","Since $A$ is stable, so long as $W$ is constant, this iteration will converge to a solution of the Lyapunov equation\n","$$S = A S A^T + W.$$\n","Also, since $A$ is stable, the steady-state mean is zero, thus we can determine the steady-state distribution in closed-form:\n","$$\\lim_{t\\rightarrow\\infty} x(t) \\sim \\mathcal{N}(0, S).$$"]},{"cell_type":"markdown","metadata":{"id":"iyhG0qxJNI3N"},"source":["In this problem, you'll make use of the facts above to build a finite MDP that approximates the LQR, solve the MDP using your algorithms from the previous problem, then make use of the facts above again to validate the solution you've obtained."]},{"cell_type":"markdown","metadata":{"id":"Ew--O5WQNI3O"},"source":["a. *Construct a finite MDP by discretizing the state and action spaces of the linear system and computing transition probabilities and transition costs to obtain  $(X,U,P,C)$ where:*\n","\n","* $X$ is a finite set of $N$ *states*;\n","* $U$ is a finite set of $M$ *actions*;\n","* $P:X\\times U\\rightarrow \\Delta(X)$ specifies transition probabilities;\n","* $C:X\\times U\\times X\\rightarrow \\mathbb{R}$ specifies transition costs."]},{"cell_type":"markdown","metadata":{"id":"vC1d1EnyNI3P"},"source":["b. *Compare the steady-state distribution of the MDP with the steady-state distribution of the linear system subject to zero-mean disturbance with covariance $W$.  Discuss how the comparison varies with respect to $|X|$.*"]},{"cell_type":"markdown","metadata":{"id":"sJmIXjP8NI3P"},"source":["Now consider the problem of minimizing the infinite-horizon discounted cost\n","$$c = \\sum_{t=0}^\\infty \\gamma^t C(x_t,u_t),$$\n","where $\\gamma\\in(0,1]$ is a *discount factor*.\n","\n","The un-discounted case $\\gamma = 1$ corresponds exactly to LQR; unfortunately, policy iteration won't converge when $\\gamma = 1$!"]},{"cell_type":"markdown","metadata":{"id":"BbyWp2KYNI3Q"},"source":["c. *Compare the optimal value and control for the MDP obtained using value or policy iteration with the optimal value and control for the original LQR problem.  Discuss how the comparison varies with respect to $|X|$, $|U|$, and $\\gamma$.*"]},{"cell_type":"markdown","metadata":{"id":"FXYHuKHmNI3R"},"source":["# paper / project presentation (4 pts)\n","\n","You will give a combined paper / project presentation during the final two weeks of class meetings.\n","\n","The paper presentation constitutes 30% of the course grade.  \n","\n","The project constitutes 50% of the course grade, with the project presentation constituting 1/5 of that portion, so 10% of the course grade.\n","\n","The presentations will be evaluated according to a simple rubric:  ***what did we learn, and how do we know we learned it?***  Your goal should be to (i) teach us something new (i.e. not covered in the course) and (ii) provide simple examples or exercises we can use to assess our understanding.  You should endeavor to adopt the notation and terminology from lecture (this may require translation from source material), and all new concepts and techniques must be defined and explained.\n","\n","Each presenter will be allotted 20min of lecture time, including answering questions that arise during or after the presentations -- ***there will be a hard stop after 20min***.  To satisfy this constraint while maximizing your expected grade, you should prepare < 10min of material for the paper portion of the presentation and < 5min of material for the project presentation.  (*Hint:* you probably need to prepare less material than you expect -- to calibrate, I encourage you to practice your presentations with a colleague who actively asks questions.)\n","\n","a. *Update your availability in the shared document accessible via Canvas -> Collaborations.*\n","\n","b. Specify whether you are planning a solo or group project -- if group, specify your group members."]}]}