{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"_mdp.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.14"}},"cells":[{"cell_type":"code","metadata":{"id":"CGXixrDHFLkU","executionInfo":{"status":"ok","timestamp":1644727668172,"user_tz":480,"elapsed":11,"user":{"displayName":"Sam Burden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjApr-2gPPda0DddsKtZiaMzZ8YWbPkAT9JP0pG=s64","userId":"06088542255363958083"}}},"source":["import numpy as np\n","import pylab as plt\n","from numpy import linalg as la\n","%matplotlib inline"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aUunj5d-FLkY"},"source":["Let $(X,U,P,L)$ be an MDP, i.e.\n","\n","* $X$ is a finite set of $n$ *states*\n","* $U$ is a finite set of $m$ *actions*\n","* $P:X\\times U\\rightarrow \\Delta(X)$ is transition probability\n","* $\\mathcal{L}:X\\times U\\times X\\rightarrow \\mathbb{R}$ is the 1-stage cost (for reward, use $-\\mathcal{L}$)\n","\n","where $\\Delta(S) = \\{p\\in[0,1]^S : \\sum_{s\\in S} p(s) = 1\\}$ is the set of probability distributions over the finite set $S$."]},{"cell_type":"code","metadata":{"id":"lYBw2lBTFLka","executionInfo":{"status":"ok","timestamp":1644728089694,"user_tz":480,"elapsed":154,"user":{"displayName":"Sam Burden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjApr-2gPPda0DddsKtZiaMzZ8YWbPkAT9JP0pG=s64","userId":"06088542255363958083"}}},"source":["n,m = 5,2\n","X = range(n)\n","U = range(m)\n","# generate transition probabilities\n","P = np.abs(np.random.randn(n,m,n))\n","P = P / P.sum(axis=2)[...,np.newaxis]\n","assert np.all(P.min(axis=2) > 0),'no probability is zero'\n","assert np.all(P.max(axis=2) < 1),'no probability is one'\n","assert np.allclose(P.sum(axis=2), 1.),'probabilities sum to 1'\n","# generate rewards\n","L = np.abs(np.random.randn(n,m,n))"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KMFStWXTFLkd"},"source":["Starting with an initial state distribution $p\\in \\Delta(X)$ and policy $\\pi:X\\rightarrow \\Delta(U)$, can simulate by iterating:\n","$$p(x)^+ = \\sum_{\\xi\\in X} p(\\xi) \\sum_{\\mu\\in U} \\pi(\\xi)(\\mu) P(\\xi,\\mu)(x).$$\n","Equivalently, defining $\\Gamma : X \\rightarrow \\Delta(X)$ by\n","$$\\forall \\xi,x\\in X : \\Gamma(\\xi)(x) = \\sum_{\\mu\\in U} \\pi(\\xi)(\\mu) P(\\xi,\\mu)(x)$$\n","and treating $p$ as a row vector, we can iterate $p^+ = p \\Gamma$."]},{"cell_type":"code","metadata":{"id":"miL_BgLOFLke","executionInfo":{"status":"ok","timestamp":1644728090524,"user_tz":480,"elapsed":150,"user":{"displayName":"Sam Burden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjApr-2gPPda0DddsKtZiaMzZ8YWbPkAT9JP0pG=s64","userId":"06088542255363958083"}}},"source":["# initial state distribution\n","p0 = np.abs(np.random.randn(n)); p0 = p0 / np.sum(p0);\n","assert np.allclose(np.sum(p0),1.)\n","# policy\n","pi = np.abs(np.random.randn(n,m)); pi = pi / np.sum(pi,axis=1)[:,np.newaxis]\n","assert np.allclose(np.sum(pi,axis=1),1.)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"0tEfZlvZFLkj","executionInfo":{"status":"ok","timestamp":1644728091090,"user_tz":480,"elapsed":3,"user":{"displayName":"Sam Burden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjApr-2gPPda0DddsKtZiaMzZ8YWbPkAT9JP0pG=s64","userId":"06088542255363958083"}}},"source":["# iterative simulation\n","def sim_p(X,U,P,R,p0,pi,T=1):\n","    n = len(X); m = len(U)\n","    p_ = [p0]\n","    for t in range(T):\n","        p  = p_[-1].copy()\n","        _p = np.nan*p\n","        for x in X:\n","            _p[x] = np.sum([[p[xi] * pi[xi,mu] * P[xi,mu,x] for mu in U] for xi in X])\n","        assert np.allclose(np.sum(_p),1.),\"simulation must preserve probabilities\"\n","        p_.append(_p)\n","    return p_\n","\n","# matrix multiplication\n","#G = np.zeros((n,n))\n","#for xi in X:\n","#    for x in X:\n","#        G[xi,x] = np.sum([pi[xi,mu] * P[xi,mu,x] for mu in U])\n","# this can actually be done in one line\n","G = np.sum(pi[...,np.newaxis]*P,axis=1)\n","assert np.allclose(np.sum(G,axis=1),1.)\n","\n","\n","# sanity check:  matrix multiplication yields same result as iterative simulation\n","# (note:  p0 is a row vector, so we're doing left multiplication on G)\n","assert np.allclose(sim_p(X,U,P,R,p0,pi)[-1], np.dot(p0,G))"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aTfhSES-FLks"},"source":["Since $\\Gamma$ is left-stochastic (row sums equal to 1) and ergodic (every state reachable from every other under policy $\\pi$), it has one unity eigenvalue, and all other eigenvalues are contractive (magnitude smaller than 1):"]},{"cell_type":"code","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1644728091741,"user":{"displayName":"Sam Burden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjApr-2gPPda0DddsKtZiaMzZ8YWbPkAT9JP0pG=s64","userId":"06088542255363958083"},"user_tz":480},"id":"JDiM86cEFLkw","outputId":"6be3ad20-4cdf-46a0-e2c9-f2b169253dd7","colab":{"base_uri":"https://localhost:8080/"}},"source":["print('spec Gamma =',la.eigvals(G))\n","assert np.all(np.abs(la.eigvals(G)[1:]) < 1)"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["spec Gamma = [ 1.        +0.j          0.10544458+0.09731843j  0.10544458-0.09731843j\n"," -0.14669747+0.06941977j -0.14669747-0.06941977j]\n"]}]},{"cell_type":"markdown","metadata":{"id":"xVNSskMhFLk4"},"source":["This implies that any initial probability distribution will asymptotically converge to the eigenvector corresponding to the unity eigenvalue, which will have no zero entries:"]},{"cell_type":"code","metadata":{"id":"MmggcbDFFLk5","executionInfo":{"status":"ok","timestamp":1644728092337,"user_tz":480,"elapsed":1,"user":{"displayName":"Sam Burden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjApr-2gPPda0DddsKtZiaMzZ8YWbPkAT9JP0pG=s64","userId":"06088542255363958083"}}},"source":["v = la.eig(G.T)[1][:,0]; v /= v.sum()\n","assert np.allclose(v,np.dot(v,G)),'v is left-eigvec of Gamma with eigval 1'\n","assert np.allclose(v, np.dot(p0,la.matrix_power(G,100))),'p0 converges to v'\n","assert np.all(v > 0.),'all states have positive probability'"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O-NUQt2HFLk_"},"source":["Let us now consider the problem of minimizing the infinite-horizon discounted cost\n","$$c(x,u) = \\sum_{t=0}^\\infty \\gamma^t \\mathcal{L}(x_t,u_t),$$\n","where $\\gamma\\in(0,1)$ is a *discount factor* and $\\mathcal{L}(x_t,u_t)$ is the cost at time $t$."]},{"cell_type":"code","metadata":{"id":"egzsWTBIFLlA","executionInfo":{"status":"ok","timestamp":1644728092933,"user_tz":480,"elapsed":3,"user":{"displayName":"Sam Burden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjApr-2gPPda0DddsKtZiaMzZ8YWbPkAT9JP0pG=s64","userId":"06088542255363958083"}}},"source":["g = np.random.rand()"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F7hCqz58FLlI"},"source":["Any policy $\\pi : X\\rightarrow\\Sigma(U)$ has an associated *value* $v^\\pi : X\\rightarrow\\mathbb{R}$ defined by\n","$$\\forall \\xi\\in X : v^\\pi(\\xi) = E[c(x,u) \\mid x_0 = \\xi]$$\n","that satisfies the *Bellman equation*\n","$$\\forall\\xi\\in X : v^\\pi(\\xi) = \\sum_{\\mu\\in U}\\pi(\\mu \\mid \\xi)\\sum_{x^+\\in X} P(x^+\\mid\\xi,\\mu)(\\mathcal{L}(\\xi,\\mu) + \\gamma\\, v^\\pi(x^+)).$$\n","Noting that $v^\\pi$ appears linearly in the above equation, we can solve for $v^\\pi$ given $\\pi$ using linear algebra:\n","$$A_\\pi v^\\pi = b_\\pi,$$\n","$$b_\\pi(\\xi) = \\sum_{\\mu\\in U} \\pi(\\mu\\mid\\xi) \\sum_{x^+\\in X} P(x^+\\mid\\xi,\\mu) \\mathcal{L}(\\xi,\\mu,x),$$\n","\n","$$A_\\pi(\\xi,x^+) = \\delta(\\xi,x^+) - \\sum_{\\mu\\in U} \\gamma\\, \\pi(\\mu\\mid\\xi) P(x^+\\mid\\xi,\\mu),$$\n","where $\\delta:X\\times X\\rightarrow\\{0,1\\}$ is the *Kronecker delta*, i.e. $\\delta(\\xi,x) = 1 \\iff \\xi = x$."]},{"cell_type":"code","metadata":{"executionInfo":{"elapsed":153,"status":"ok","timestamp":1644728110225,"user":{"displayName":"Sam Burden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjApr-2gPPda0DddsKtZiaMzZ8YWbPkAT9JP0pG=s64","userId":"06088542255363958083"},"user_tz":480},"id":"xmOMJdBzFLlL","outputId":"027f1b97-b66f-4cac-84a9-c534405430df","colab":{"base_uri":"https://localhost:8080/"}},"source":["def policy_evaluation(X,U,P,L,g,pi):\n","    b = np.asarray([ np.sum([[pi[xi,mu] * P[xi,mu,x] * L[xi,mu,x] \n","                          for x in X] for mu in U]) for xi in X])\n","    I = np.identity(n)\n","    A = I - np.asarray([ np.asarray([ np.sum([g * pi[xi,mu] * P[xi,mu,x] \n","                                              for mu in U]) for x in X]) for xi in X])\n","    # v satisfies Bellman equation\n","    v = np.dot(b,la.inv(A).T)\n","    assert np.allclose(v, [np.sum([[pi[xi,mu] * P[xi,mu,x] * (L[xi,mu,x] + g*v[x]) \n","                                for x in X] for mu in U]) for xi in X])\n","    return v\n","print('value of policy:')\n","print(policy_evaluation(X,U,P,L,g,pi))"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["value of policy:\n","[19.84294402 19.6297296  20.36225594 19.83700439 19.74344903]\n"]}]},{"cell_type":"markdown","metadata":{"id":"2Pi6eAiDFLlT"},"source":["Given the value $v^\\pi:X\\rightarrow\\mathbb{R}$ of any policy $\\pi:X\\rightarrow\\Delta(U)$, we can perform *policy improvement* via:\n","$$\\forall\\xi\\in X : \\pi^+(\\xi) = \\arg\\min_{\\mu\\in U}\\sum_{x^+\\in X} P(x^+\\mid\\xi,\\mu)(\\mathcal{L}(\\xi,\\mu) + \\gamma v^\\pi(x^+)).$$\n","(Note that there is a slight abuse of notation here since $\\pi^+(\\xi)\\in\\Delta(U)$ not $\\pi^+(\\xi)\\in U$; the equation should be interpreted as specifying a deterministic policy.)"]},{"cell_type":"code","metadata":{"id":"Ax4hoUw-FLlV","executionInfo":{"status":"ok","timestamp":1644728229791,"user_tz":480,"elapsed":142,"user":{"displayName":"Sam Burden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjApr-2gPPda0DddsKtZiaMzZ8YWbPkAT9JP0pG=s64","userId":"06088542255363958083"}}},"source":["def policy_improvement(X,U,P,L,g,v):\n","    n = len(X); m = len(U)\n","    pi = np.zeros((n,m))\n","    for xi in X:\n","        u = np.argmin([np.sum([P[xi,mu,x]*(L[xi,mu,x] + g*v[x]) for x in X]) for mu in U])\n","        #assert len(u) == 1\n","        pi[xi,u] = 1.\n","    return pi"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"executionInfo":{"elapsed":139,"status":"ok","timestamp":1644728246565,"user":{"displayName":"Sam Burden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjApr-2gPPda0DddsKtZiaMzZ8YWbPkAT9JP0pG=s64","userId":"06088542255363958083"},"user_tz":480},"id":"ON_z6w6dFLlb","outputId":"61194083-7417-411b-c392-a4ffeee0daa9","colab":{"base_uri":"https://localhost:8080/"}},"source":["print('improved policy:')\n","print(policy_improvement(X,U,P,L,g,policy_evaluation(X,U,P,L,g,pi)))"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["improved policy:\n","[[1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"oOJBt-23FLlh"},"source":["Iteratively evaluating and improving the policy defines a *policy iteration* algorithm that converges to the optimal deterministic policy:"]},{"cell_type":"code","metadata":{"executionInfo":{"elapsed":240,"status":"ok","timestamp":1644728442956,"user":{"displayName":"Sam Burden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjApr-2gPPda0DddsKtZiaMzZ8YWbPkAT9JP0pG=s64","userId":"06088542255363958083"},"user_tz":480},"id":"sufXCj1oFLlm","outputId":"cedd3e23-cb1f-4514-9dbc-b7775de74fec","colab":{"base_uri":"https://localhost:8080/"}},"source":["def policy_iteration(X,U,P,L,g,pi0):\n","    pi_ = [pi0]\n","    v_  = []\n","    while len(v_) < 2 or not(np.allclose(v_[-1],v_[-2])):\n","        pi = pi_[-1]\n","        v  = policy_evaluation(X,U,P,L,g,pi)\n","        _pi = policy_improvement(X,U,P,L,g,v)\n","        pi_.append(_pi)\n","        v_.append(v)\n","    return pi_,v_\n","\n","pi_PI,v_PI = policy_iteration(X,U,P,L,g,pi)\n","print('%d iterations' % len(v_PI))\n","print('optimal policy:')\n","print(pi_PI[-1])\n","print('optimal value:')\n","print(np.asarray(v_PI[-1]))"],"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["3 iterations\n","optimal policy:\n","[[1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]]\n","optimal value:\n","[12.99380609 13.30836604 13.23951656 13.42796287 13.30285835]\n"]}]},{"cell_type":"markdown","metadata":{"id":"T5R3TbwAFLlv"},"source":["Rather than evaluate the value of the policy, we can iterate toward the optimal policy and value simultaneously:\n","$$\\forall\\xi\\in X : v_{j+1}(\\xi) = \\sum_{\\mu\\in U}\\pi_j(\\mu\\mid\\xi)\\sum_{x^+\\in X} P(x^+\\mid\\xi,\\mu)(\\mathcal{L}(\\xi,\\mu) + \\gamma\\, v_j(x^+)).$$\n","$$\\forall\\xi\\in X : \\pi_{j+1}(\\xi) = \\arg\\min_{\\mu\\in U}\\sum_{x^+\\in X} P(x^+\\mid\\xi,\\mu)(\\mathcal{L}(\\xi,\\mu) + \\gamma\\, v_{j+1}(x^+)].$$"]},{"cell_type":"code","metadata":{"id":"Shxa5AQ4FLlv","executionInfo":{"status":"ok","timestamp":1644728352430,"user_tz":480,"elapsed":170,"user":{"displayName":"Sam Burden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjApr-2gPPda0DddsKtZiaMzZ8YWbPkAT9JP0pG=s64","userId":"06088542255363958083"}}},"source":["def value_update(X,U,P,R,g,pi,V):\n","    return np.asarray([np.sum([[pi[xi,mu] * P[xi,mu,x] * (R[xi,mu,x] + g * V[x]) \n","                                for x in X] for mu in U]) for xi in X])"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"executionInfo":{"elapsed":141,"status":"ok","timestamp":1644728445535,"user":{"displayName":"Sam Burden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjApr-2gPPda0DddsKtZiaMzZ8YWbPkAT9JP0pG=s64","userId":"06088542255363958083"},"user_tz":480},"id":"AVe2u_kfFLly","outputId":"6b4c8c65-9a9c-40fd-8943-7aaabfd297df","colab":{"base_uri":"https://localhost:8080/"}},"source":["def value_iteration(X,U,P,L,g,pi0,maxiter=100):\n","    n = len(X)\n","    pi_ = [pi0]\n","    v_  = [np.zeros(n)]\n","    while len(v_) < 2 or (not(np.allclose(v_[-1],v_[-2])) and len(v_) < maxiter):\n","        pi = pi_[-1]\n","        v  = v_[-1]\n","        _v = value_update(X,U,P,L,g,pi,v)\n","        _pi = policy_improvement(X,U,P,R,g,_v)\n","        pi_.append(_pi)\n","        v_.append(_v)\n","    return pi_,v_\n","\n","pi_VI,v_VI = value_iteration(X,U,P,L,g,pi,200)\n","print('%d iterations' % len(v_VI))\n","print('optimal policy:')\n","print(pi_VI[-1])\n","print('optimal value:')\n","print(np.asarray(v_VI[-1]))"],"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["171 iterations\n","optimal policy:\n","[[1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [0. 1.]]\n","optimal value:\n","[14.33347547 14.57269701 14.56239312 14.65294452 14.79329608]\n"]}]},{"cell_type":"code","metadata":{"executionInfo":{"elapsed":165,"status":"ok","timestamp":1644728425826,"user":{"displayName":"Sam Burden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjApr-2gPPda0DddsKtZiaMzZ8YWbPkAT9JP0pG=s64","userId":"06088542255363958083"},"user_tz":480},"id":"6l_LJkLOFLl4","outputId":"3dad087e-60cd-4772-b594-caa7bb289a15","colab":{"base_uri":"https://localhost:8080/"}},"source":["print('policy discrepancy between VI and PI = %d'%np.max(np.abs(pi_VI[-1] - pi_PI[-1])))\n","print('value discrepancy between VI and PI = %0.1e'%np.max(np.abs(v_VI[-1] - v_PI[-1])))"],"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["policy discrepancy between VI and PI = 1\n","value discrepancy between VI and PI = 1.5e+00\n"]}]},{"cell_type":"markdown","metadata":{"id":"56EVCyI3FLl-"},"source":["Anticipating future methods that learn optimal policies without access to a system model, we formulate policy and value iteration using the *quality* function (also called *action-value* function) $Q:X\\times U\\rightarrow\\mathbb{R}$ defined by:\n","$$\\forall \\xi\\in X, \\mu\\in U : Q^\\pi(\\xi,\\mu) = \\sum_{x\\in X} P(\\xi,\\mu)(x)[R(\\xi,\\mu,x) + \\gamma V^\\pi(x)].$$\n","If $\\pi$ is optimal, then $Q^\\pi$ is related to $V^\\pi$ and $\\pi$ via:\n","$$\\forall \\xi\\in X: V^\\pi(\\xi) = \\min_{\\mu\\in U} Q^\\pi(\\xi,\\mu),\\ \\pi(\\xi) = \\arg\\min_{\\mu\\in U} Q^\\pi(\\xi,\\mu).$$"]},{"cell_type":"markdown","metadata":{"id":"NRluRmJJFLl_"},"source":["### Q function policy iteration (11.3-43,44 in LewisVrabieSyrmos2008)\n","$$\\forall\\xi\\in X,\\mu\\in U : Q_j(\\xi,\\mu) = \\sum_{x\\in X} P(\\xi,\\mu)(x)[R(\\xi,\\mu,x) + \\gamma Q_j(x,\\pi_j(x))].$$\n","\n","$$\\forall\\xi\\in X : \\pi_{j+1}(\\xi) = \\arg\\min_{\\mu\\in U} Q_j(\\xi,\\mu).$$\n","\n","**This one is more annoying to implement, so I skipped it; I also can't actually make sense of the formula for a nondeterministic policy, due to the \\pi_j(x) dependence in the right-hand-side.**\n","\n","**Here's a proposed alternative formulation that seems to make sense for nondeterministic policies, but the corresponding code doesn't seem to work...**\n","\n","$$\\forall\\xi\\in X,\\mu\\in U : Q_j(\\xi,\\mu) = \\sum_{x\\in X} P(\\xi,\\mu)(x)\\left[R(\\xi,\\mu,x) + \\gamma \\sum_{u\\in U}\\pi_j(x,u) Q_j(x,u)\\right].$$\n","\n","$$\\forall\\xi\\in X : \\pi_{j+1}(\\xi) = \\arg\\min_{\\mu\\in U} Q_j(\\xi,\\mu).$$"]},{"cell_type":"markdown","metadata":{"id":"91-CxDaDFLmA"},"source":["### quality function value iteration (11.3-45,46 in LewisVrabieSyrmos2008)\n","Just like with *policy iteration* above, iteratively evaluating and improving the policy using the $q$ function defines a *policy iteration* algorithm:\n","$$\\forall\\xi\\in X,\\mu\\in U : q_{j+1}(\\xi,\\mu) = \\sum_{x^+\\in X} P(x^+\\mid\\xi,\\mu)(\\mathcal{L}(\\xi,\\mu) + \\gamma\\, q_j(x,\\pi_j(x)).$$\n","\n","$$\\forall\\xi\\in X : \\pi_{j+1}(\\xi) = \\arg\\min_{\\mu\\in U} q_{j+1}(\\xi,\\mu).$$"]},{"cell_type":"code","metadata":{"id":"2sBOZKdyFLmB","executionInfo":{"status":"ok","timestamp":1644728548198,"user_tz":480,"elapsed":6,"user":{"displayName":"Sam Burden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjApr-2gPPda0DddsKtZiaMzZ8YWbPkAT9JP0pG=s64","userId":"06088542255363958083"}}},"source":["def value_update_q(X,U,P,l,g,pi,q):\n","    \"\"\"only correct for deterministic policies\"\"\"\n","    _pi = [np.argmax(pix) for pix in pi]\n","    return np.asarray([[np.sum([P[xi,mu,x] * (L[xi,mu,x] + g * q[x,_pi[x]]) \n","                                for x in X]) for mu in U] for xi in X])"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1644728574574,"user":{"displayName":"Sam Burden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjApr-2gPPda0DddsKtZiaMzZ8YWbPkAT9JP0pG=s64","userId":"06088542255363958083"},"user_tz":480},"id":"oYQCOdlAFLmF","outputId":"538b52f2-c45e-4181-833c-c4682cdb0db4","colab":{"base_uri":"https://localhost:8080/"}},"source":["_pi = pi_VI[-1]\n","\n","print ('policy value:')\n","print (policy_evaluation(X,U,P,L,g,_pi))\n","\n","q = np.zeros((n,m))\n","for _ in range(10):\n","    q = value_update_q(X,U,P,L,g,_pi,q)\n","print ('policy value from q:')\n","print (q.min(axis=1))"],"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["policy value:\n","[14.33613249 14.57535403 14.56505014 14.65560154 14.7959531 ]\n","policy value from q:\n","[5.52332784 5.76254935 5.75224558 5.84279717 5.74907439]\n"]}]},{"cell_type":"code","metadata":{"id":"7BCb_mCHFLmL","executionInfo":{"status":"ok","timestamp":1644728590021,"user_tz":480,"elapsed":152,"user":{"displayName":"Sam Burden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjApr-2gPPda0DddsKtZiaMzZ8YWbPkAT9JP0pG=s64","userId":"06088542255363958083"}}},"source":["def policy_improvement_q(X,U,P,L,g,q):\n","    n = len(X); m = len(U)\n","    pi = np.zeros((n,m))\n","    for xi in X:\n","        u = np.argmin(q[xi])\n","        pi[xi,u] = 1.\n","    return pi"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1644728682071,"user":{"displayName":"Sam Burden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjApr-2gPPda0DddsKtZiaMzZ8YWbPkAT9JP0pG=s64","userId":"06088542255363958083"},"user_tz":480},"id":"AuhfcC7dFLmO","outputId":"5ddbae62-b7c1-430b-add7-a15e64a14a15","colab":{"base_uri":"https://localhost:8080/"}},"source":["def value_iteration_q(X,U,P,L,g,pi0,maxiter=100):\n","    n = len(X); m = len(U)\n","    pi_ = [pi0]\n","    q_  = [np.zeros((n,m))]\n","    while len(q_) < 2 or (not(np.allclose(q_[-1],q_[-2])) and len(q_) < maxiter):\n","        pi = pi_[-1]\n","        q  = q_[-1]\n","        _q = value_update_q(X,U,P,L,g,pi,q)\n","        _pi = policy_improvement_q(X,U,P,L,g,_q)\n","        pi_.append(_pi)\n","        q_.append(_q)\n","    return pi_,q_\n","\n","pi_VI_q,q_VI = value_iteration_q(X,U,P,L,g,pi,200)\n","print ('%d iterations with q' % len(q_VI))\n","print ('optimal policy from q:')\n","print (pi_VI[-1])\n","print ('optimal value from q:')\n","print (q_VI[-1].min(axis=1))\n","\n","print ('policy discrepancy between VI and VI with q = %d'%np.max(np.abs(pi_VI[-1] - pi_VI_q[-1])))\n","print ('value discrepancy between VI and VI with q = %0.1e'%np.max(np.abs(v_VI[-1] - q_VI[-1].min(axis=1))))"],"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["171 iterations with q\n","optimal policy from q:\n","[[1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [1. 0.]\n"," [0. 1.]]\n","optimal value from q:\n","[12.99134795 13.3059079  13.23705842 13.42550472 13.30040021]\n","policy discrepancy between VI and VI with q = 1\n","value discrepancy between VI and VI with q = 1.5e+00\n"]}]},{"cell_type":"markdown","metadata":{"id":"Supqj6UmFLmT"},"source":["### Fig 5.1 in Sutton Barto 1998\n","\n","When the system model (i.e. $P$ and $R$) is not known but trajectory episodes (i.e. $\\{(\\xi_t^e,\\mu_t^e,x_t^e,r_t^e)\\mid t\\in T,e\\in E\\}$) are available, can evaluate the policy using \"Monte Carlo\" methods -- simply compute the average observed discounted future reward for each state.\n","\n","**Note:** it's important to only include discounted future reward from the first occurence of each state in each simulation trajectory.  (Not sure if violating this slows or prevents convergence . . .)"]},{"cell_type":"code","metadata":{"id":"8H4dkf_qFLmV","executionInfo":{"status":"ok","timestamp":1644728729575,"user_tz":480,"elapsed":164,"user":{"displayName":"Sam Burden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjApr-2gPPda0DddsKtZiaMzZ8YWbPkAT9JP0pG=s64","userId":"06088542255363958083"}}},"source":["def sim(X,U,P,R,x0,pi,T=1):\n","    n = len(X); m = len(U)\n","    trj = [[None,None,x0,0.]]\n","    for t in range(T):\n","        xi = trj[-1][2]\n","        mu = np.random.choice(U,p=pi[xi])\n","        x  = np.random.choice(X,p=P[xi,mu])\n","        r  = R[xi,mu,x]\n","        trj.append([xi,mu,x,r])\n","    return trj[1:]"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"sXsJyugMFLma","executionInfo":{"status":"ok","timestamp":1644728816055,"user_tz":480,"elapsed":4,"user":{"displayName":"Sam Burden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjApr-2gPPda0DddsKtZiaMzZ8YWbPkAT9JP0pG=s64","userId":"06088542255363958083"}}},"source":["def mc_policy_evaluation(X,trjs,g):\n","    n = len(X)\n","    V = np.zeros(n)\n","    N = np.zeros(n)\n","    for trj in trjs:\n","        x_ = []\n","        T = len(trj)\n","        r_ = np.asarray(trj)[:,-1]\n","        g_ = g**np.asarray(range(T))\n","        for t,(xi,mu,x,r) in enumerate(trj):\n","            if xi not in x_:\n","                V[xi] += np.sum(r_[t:]*g_[:T-t])\n","                N[xi] += 1\n","                x_.append(xi)\n","    return V / N\n","\n","def mc_policy_evaluation_q(X,trjs,g):\n","    n = len(X); m = len(U)\n","    q = np.zeros((n,m))\n","    N = np.zeros((n,m))\n","    for trj in trjs:\n","        xu_ = []\n","        T = len(trj)\n","        r_ = np.asarray(trj)[:,-1]\n","        g_ = g**np.asarray(range(T))\n","        for t,(xi,mu,x,r) in enumerate(trj):\n","            if (xi,mu) not in xu_:\n","                q[xi,mu] += np.sum(r_[t:]*g_[:T-t])\n","                N[xi,mu] += 1\n","                xu_.append((xi,mu))\n","    return q / (N+1)"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"HHM17_8zFLmk","executionInfo":{"status":"ok","timestamp":1644728805808,"user_tz":480,"elapsed":4256,"user":{"displayName":"Sam Burden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjApr-2gPPda0DddsKtZiaMzZ8YWbPkAT9JP0pG=s64","userId":"06088542255363958083"}}},"source":["E = 2000 # number of episodes\n","T = 20 # horizon of episodes\n","trjs = [sim(X,U,P,L,np.random.choice(X),_pi,T=T) for e in range(E)]"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"MVi8ti2GFLmn","outputId":"c7d670f1-0c8a-499a-e64d-330cdd1459e6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644728818370,"user_tz":480,"elapsed":1032,"user":{"displayName":"Sam Burden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjApr-2gPPda0DddsKtZiaMzZ8YWbPkAT9JP0pG=s64","userId":"06088542255363958083"}}},"source":["print ('value of policy:')\n","print (policy_evaluation(X,U,P,R,g,_pi))\n","print ('mc value of policy:')\n","print (mc_policy_evaluation(X,trjs,g))\n","print ('mc value of policy with q:')\n","print (mc_policy_evaluation_q(X,trjs,g).max(axis=1)) # hack b/c policy is deterministic\n","print ('q:')\n","print (q)\n","print ('mc q:')\n","print (mc_policy_evaluation_q(X,trjs,g))"],"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["value of policy:\n","[15.18830653 15.32233441 15.58651968 16.29293235 15.15343541]\n","mc value of policy:\n","[7.85385794 7.86677385 7.91296371 8.19586907 8.76430418]\n","mc value of policy with q:\n","[7.84989135 7.86276428 7.90894289 8.191736   8.75992422]\n","q:\n","[[5.52332784 6.08842241]\n"," [5.76254935 5.91891724]\n"," [5.75224558 7.10828245]\n"," [5.84279717 6.03036829]\n"," [5.74907439 5.98314866]]\n","mc q:\n","[[7.84989135 0.        ]\n"," [7.86276428 0.        ]\n"," [7.90894289 0.        ]\n"," [8.191736   0.        ]\n"," [0.         8.75992422]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"0xXjLy_xFLms"},"source":["For a nondeterministic policy, using the same number of samples as above is clearly insufficient:"]},{"cell_type":"code","metadata":{"id":"Qa06qD-CFLmt","executionInfo":{"status":"ok","timestamp":1644728843750,"user_tz":480,"elapsed":4616,"user":{"displayName":"Sam Burden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjApr-2gPPda0DddsKtZiaMzZ8YWbPkAT9JP0pG=s64","userId":"06088542255363958083"}}},"source":["E = 2000 # number of episodes\n","T = 20 # horizon of episodes\n","trjs = [sim(X,U,P,L,np.random.choice(X),pi,T=T) for e in range(E)]"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"id":"uFwlGh5dFLmv","outputId":"b385dbdf-5883-41cb-f917-44a472a1f362","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644728854830,"user_tz":480,"elapsed":757,"user":{"displayName":"Sam Burden","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjApr-2gPPda0DddsKtZiaMzZ8YWbPkAT9JP0pG=s64","userId":"06088542255363958083"}}},"source":["print ('value of policy:')\n","print (policy_evaluation(X,U,P,L,g,pi))\n","print ('mc value of policy:')\n","print (mc_policy_evaluation(X,trjs,g))\n","print ('mc value of policy with Q:')\n","#print mc_policy_evaluation_Q(X,trjs,g).min(axis=1)\n","print (np.sum(mc_policy_evaluation_Q(X,trjs,g)*pi,axis=1))"],"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["value of policy:\n","[19.84294402 19.6297296  20.36225594 19.83700439 19.74344903]\n","mc value of policy:\n","[10.63093643 10.50900911 10.85459998 11.19996931 11.1746593 ]\n","mc value of policy with Q:\n","[10.39445666  9.36022822  9.71960217 11.15216371 10.34870746]\n"]}]},{"cell_type":"markdown","metadata":{"id":"jJCd1BvPFLm1"},"source":["**I'm currently unsure whether I'm using insufficient number or horizon of episodes, have a bug in my code, or this scheme simply doesn't converge for nondeterminstic policies.**"]}]}